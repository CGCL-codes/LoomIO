meta:
- desc: |
   Run ceph on two nodes, using one of them as a client,
   with a separate client-only node. 
   Use xfs beneath the osds.
   install ceph/luminous v12.2.2 point version
   run workload and upgrade-sequence in parallel
   install ceph/luminous v12.2.5 point version
   run workload and upgrade-sequence in parallel
   install ceph/luminous v12.2.7 point version
   run workload and upgrade-sequence in parallel
   install ceph/luminous v12.2.8 point version
   run workload and upgrade-sequence in parallel
   install ceph/luminous v12.2.9 point version
   run workload and upgrade-sequence in parallel
   install ceph/luminous v12.2.10 point version
   run workload and upgrade-sequence in parallel

   install ceph/luminous latest version
   run workload and upgrade-sequence in parallel
overrides:
  ceph:
    log-whitelist:
    - reached quota
    - scrub
    - osd_map_max_advance
    - wrongly marked
    - FS_DEGRADED
    - POOL_APP_NOT_ENABLED
    - CACHE_POOL_NO_HIT_SET
    - POOL_FULL
    - SMALLER_PG
    - pool\(s\) full
    - OSD_DOWN
    - missing hit_sets
    - CACHE_POOL_NEAR_FULL
    - PG_AVAILABILITY
    - PG_DEGRADED
    - application not enabled
    - overall HEALTH_
    fs: xfs
    conf:
      mon:
        mon debug unsafe allow tier with nonempty snaps: true
        mon warn on pool no app: false
      osd:
        osd map max advance: 1000
        osd_class_load_list: "cephfs hello journal lock log numops rbd refcount 
                              replica_log rgw sdk statelog timeindex user version"
        osd_class_default_list: "cephfs hello journal lock log numops rbd refcount 
                                 replica_log rgw sdk statelog timeindex user version"
      client:
        rgw_crypt_require_ssl: false
        rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo= testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
roles:
- - mon.a
  - mds.a
  - osd.0
  - osd.1
  - osd.2
  - mgr.x
- - mon.b
  - mon.c
  - osd.3
  - osd.4
  - osd.5
  - client.0
- - client.1
openstack:
- volumes: # attached to each instance
    count: 3
    size: 30 # GB
tasks:
- print: "****  v12.2.2 about to install"
- install:
    tag: v12.2.2
    # line below can be removed its from jewel test
    #exclude_packages: ['ceph-mgr','libcephfs2','libcephfs-devel','libcephfs-dev', 'librgw2']
- print: "**** done v12.2.2 install"
- ceph:
   fs: xfs
   add_osds_to_crush: true
- print: "**** done ceph xfs"
- sequential:
   - workload
- print: "**** done workload v12.2.2"

####  upgrade to v12.2.5
- install.upgrade:
    #exclude_packages: ['ceph-mgr','libcephfs2','libcephfs-devel','libcephfs-dev']
    mon.a:
      tag: v12.2.5
    mon.b:
      tag: v12.2.5
    # Note that client.a IS NOT upgraded at this point
- parallel:
   - workload_luminous
   - upgrade-sequence_luminous
- print: "**** done parallel luminous v12.2.5"

####  upgrade to v12.2.7
- install.upgrade:
    #exclude_packages: ['ceph-mgr','libcephfs2','libcephfs-devel','libcephfs-dev']
    mon.a:
      tag: v12.2.7
    mon.b:
      tag: v12.2.7
    # Note that client.a IS NOT upgraded at this point
- parallel:
   - workload_luminous
   - upgrade-sequence_luminous
- print: "**** done parallel luminous v12.2.7"

####  upgrade to v12.2.8
- install.upgrade:
    #exclude_packages: ['ceph-mgr','libcephfs2','libcephfs-devel','libcephfs-dev']
    mon.a:
      tag: v12.2.8
    mon.b:
      tag: v12.2.8
    # Note that client.a IS NOT upgraded at this point
- parallel:
   - workload_luminous
   - upgrade-sequence_luminous
- print: "**** done parallel luminous v12.2.8"


####  upgrade to v12.2.9
- install.upgrade:
    #exclude_packages: ['ceph-mgr','libcephfs2','libcephfs-devel','libcephfs-dev']
    mon.a:
      tag: v12.2.9
    mon.b:
      tag: v12.2.9
    # Note that client.a IS NOT upgraded at this point
- parallel:
   - workload_luminous
   - upgrade-sequence_luminous
- print: "**** done parallel luminous v12.2.9"

####  upgrade to v12.2.10
- install.upgrade:
    #exclude_packages: ['ceph-mgr','libcephfs2','libcephfs-devel','libcephfs-dev']
    mon.a:
      tag: v12.2.10
    mon.b:
      tag: v12.2.10
    # Note that client.a IS NOT upgraded at this point
- parallel:
   - workload_luminous
   - upgrade-sequence_luminous
- print: "**** done parallel luminous v12.2.10"


####  upgrade to latest luminous
- install.upgrade:
    #exclude_packages: ['ceph-mgr','libcephfs2','libcephfs-devel','libcephfs-dev']
    mon.a:
    mon.b:
    # Note that client.a IS NOT upgraded at this point
- parallel:
   - workload_luminous
   - upgrade-sequence_luminous
- print: "**** done parallel luminous branch"

#######################
workload:
   sequential:
   - workunit:
       clients:
         client.0:
           - suites/blogbench.sh
workload_luminous:
   full_sequential:
   - workunit:
       tag: v12.2.2
       clients:
         client.1:
         - rados/test.sh
         - cls
       env:
         CLS_RBD_GTEST_FILTER: '*:-TestClsRbd.snapshots_namespaces'
   - print: "**** done rados/test.sh &  cls workload_luminous"
   - sequential:
     - rgw: [client.0]
     - print: "**** done rgw workload_luminous"
     - s3tests:
         client.0:
           force-branch: ceph-luminous
           rgw_server: client.0
           scan_for_encryption_keys: false
     - print: "**** done s3tests workload_luminous"
upgrade-sequence_luminous:
   sequential:
   - print: "**** done branch: luminous install.upgrade"
   - ceph.restart: [mds.a]
   - sleep:
       duration: 60
   - ceph.restart: [osd.0]
   - sleep:
       duration: 30
   - ceph.restart: [osd.1]
   - sleep:
       duration: 30
   - ceph.restart: [osd.2]
   - sleep:
       duration: 30
   - ceph.restart: [osd.3]
   - sleep:
       duration: 30
   - ceph.restart: [osd.4]
   - sleep:
       duration: 30
   - ceph.restart: [osd.5]
   - sleep:
       duration: 60
   - ceph.restart: [mon.a]
   - sleep:
       duration: 60
   - ceph.restart: [mon.b]
   - sleep:
       duration: 60
   - ceph.restart: [mon.c]
   - sleep:
       duration: 60
   - print: "**** done ceph.restart all luminous branch mds/osd/mon"
